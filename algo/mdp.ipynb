{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP (Markov Decision Process)\n",
    "“Markov” generally means that given the present state, the future and the past are independent<br>\n",
    "For Markov decision processes, “Markov” means action outcomes depend only on the current state<br>\n",
    "This is just like search, where the successor function could only depend on the current state (not the history)<br>\n",
    "MDP are characterized by:\n",
    "* Set of states S\n",
    "* Start state s0\n",
    "* Set of actions A\n",
    "* Transitions P(s’|s,a) (or T(s,a,s’))\n",
    "* Rewards R(s,a,s’) (and discount )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us have a gridworld\n",
    "#ref: Chapter 17, Artificial Intelligence a Modern Approach\n",
    "#ref: CS188 https://inst.eecs.berkeley.edu/~cs188/fa19/\n",
    "#ref: https://inst.eecs.berkeley.edu/~cs188/fa19/assets/slides/lec8.pdf\n",
    "#ref: https://courses.cs.washington.edu/courses/cse473/13au/slides/17-mdp-rl.pdf\n",
    "\n",
    "#This class will create a 2D grid of row x colums \n",
    "#Some of the cells can be disabled by putting it into walls\n",
    "#cells are addressed just like 2d arrays (r,c)\n",
    "#There are possibly many terminal states\n",
    "#terminal states have only one action available: Exit \n",
    "#Transision is set by default as 80% action and 20%sideways ( a variable noise is used to control this distribution)\n",
    "#There is a special end state, (-1,-1), from which NO action is available. This state is used as a final state.\n",
    "\n",
    "#Actions #just some alias\n",
    "Up    = 0\n",
    "Down  = 1\n",
    "Right = 2\n",
    "Left  = 3\n",
    "Exit  = 4\n",
    "\n",
    "class GridWorld :\n",
    "    #Default is as given in the AIMA book\n",
    "    def __init__(self, \n",
    "                 rows    =3, \n",
    "                 columns =4, \n",
    "                 walls   =[(1,1)], terminals= {(0,3):+1.0, (1,3):-1.0}, \n",
    "                 gamma   =1.0, \n",
    "                 living_reward=0,\n",
    "                 noise   =0.2\n",
    "                ) :\n",
    "        \"\"\"We dont expect these parameters to change during the agent run\"\"\"\n",
    "        self.rows      = rows\n",
    "        self.columns   = columns\n",
    "        self.N         = rows * columns #total cells\n",
    "        self.walls     = walls\n",
    "        self.terminals = terminals #dictionary of terminal celss and their rewards.\n",
    "        self.gamma     = gamma\n",
    "        self.living_reward = living_reward\n",
    "        self.all_actions   = [ Up, Down, Right, Left, Exit ]\n",
    "        self.end_state     = (-1, -1) #a dummy state to reach after taking Exit\n",
    "        self.all_states    = [(r,c) for r in range(rows) for c in range(columns) if (r,c) not in walls ] + [self.end_state]\n",
    "        self.noise         = noise\n",
    "        \n",
    "        \n",
    "        #transitions from each state and the probabilities\n",
    "        self.noise                = noise\n",
    "        self.action_transitions   = { \n",
    "            Up:   ([Up,    Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Down: ([Down,  Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Left: ([Left,  Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Right:([Right, Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Exit :([Exit], [1.0])\n",
    "        }\n",
    "    \n",
    "    def actions(self, state) :\n",
    "        \"\"\"returns all valid actions from the current state\"\"\"\n",
    "        if state in self.terminals :\n",
    "            return [Exit]\n",
    "        if state == self.end_state :\n",
    "            return [] #No action available.\n",
    "        return [ Up, Down, Right, Left ]\n",
    "    \n",
    "    def reward(self, state, action, next_state=None) :\n",
    "        \"\"\"reward is the instantaneous reward. It is usually R(s,a,s')\"\"\"\n",
    "        #In grid world the reward depends only on state.\n",
    "        if state in self.terminals :\n",
    "            return self.terminals[state] #dict has the terminal values +1 or -1\n",
    "        if state == self.end_state :\n",
    "            return 0.0\n",
    "        return self.living_reward        #usually a small -ve value\n",
    "    \n",
    "    def transitions(self, state, action) :\n",
    "        \"\"\"return a list of tuple(nextstate, action, probability)\"\"\"\n",
    "        actual_actions, probs = self.action_transitions[action]\n",
    "        return [ self._next_cell(state, a) for a in actual_actions ], actual_actions, probs\n",
    "    \n",
    "    def move(self, state, action) :\n",
    "        \"\"\"Take the action and return the tuple(new_state, reward, is_terminal)\"\"\"                          \n",
    "        assert action in self.actions(state) #just a check if this is a valid action at this time or not\n",
    "        \n",
    "        cells, actions, p = self.transitions(state, action)\n",
    "        \n",
    "        #we choose one cell acccording to probabilities\n",
    "        new_state   = random.choices(cells, weights=p)[0] #only one; we take index 0                \n",
    "        reward      = self.reward(state, action) #\n",
    "        \n",
    "        is_terminal = False\n",
    "        if new_state == self.end_state :\n",
    "            is_terminal = True\n",
    "            \n",
    "        return new_state, reward, is_terminal #keep the same for mat as OpenAI gym.\n",
    "    \n",
    "    def _next_cell(self, state, action) : \n",
    "        \"\"\"Blindly takes the action without checking anything and returns the position\"\"\"\n",
    "        r,c = state #row & column\n",
    "        if action == Exit :\n",
    "            return self.end_state\n",
    "        if action == Up :\n",
    "            target = r-1, c  \n",
    "        if action == Down :\n",
    "            target = r+1, c\n",
    "        if action == Right :\n",
    "            target = r, c+1  \n",
    "        if action == Left :\n",
    "            target = r, c-1 \n",
    "        \n",
    "        if self._valid_cell(target) :\n",
    "            return target\n",
    "        return state #stay put the target is invalid.\n",
    "    \n",
    "    def _valid_cell(self, cell) :\n",
    "        \"\"\"Returns true if the cell is a valid cell\"\"\"\n",
    "        r, c = cell #this may be an illegal node; we need to check\n",
    "        \n",
    "        #is it any of the walls?\n",
    "        if (r,c) in self.walls :\n",
    "            return False\n",
    "        \n",
    "        #is it outside the grid?\n",
    "        if r < 0 or r >= self.rows or c < 0 or c >= self.columns :\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    #pretty print the grid and agent if given.\n",
    "    def print(self, agent_state=None) :\n",
    "        for r in range(self.rows) :\n",
    "            for c in range(self.columns) :\n",
    "                cell = (r,c)\n",
    "                if cell in self.walls :\n",
    "                    print('# ', end='')\n",
    "                elif cell in self.terminals :\n",
    "                    if self.terminals[cell] > 0 :\n",
    "                        print('+', end=' ')\n",
    "                    else :\n",
    "                        print('-', end=' ')\n",
    "                elif cell == agent_state :\n",
    "                    print('@ ', end='')\n",
    "                else :\n",
    "                    print('. ', end='')\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple class to hold the policy dictionary\n",
    "#useful for printing the policy and hiding some details.\n",
    "\n",
    "class Policy :\n",
    "    def __init__(self, grid_world=None) :\n",
    "        \"\"\"Holds one policy and returns actions according to it\"\"\"\n",
    "        self.grid_world = grid_world\n",
    "        self.policy     = { } #{ state: policy_action}\n",
    "        \n",
    "    def __getitem__(self, state) :\n",
    "        return self.policy[state]\n",
    "    \n",
    "    def __setitem__(self, state, action) :\n",
    "        self.policy[state] = action\n",
    "    \n",
    "    \n",
    "    #Just a pretty print function for easy debugging\n",
    "    def print(self) :\n",
    "        print_chars = {Up:'^', Down:'v', Right:'>', Left:'<', Exit:'+'}\n",
    "        for state in [(r,c) for r in range(self.grid_world.rows) for c in range(self.grid_world.columns)]:\n",
    "            \n",
    "            if state in self.grid_world.terminals :\n",
    "                if self.grid_world.terminals[state] >= 0 :\n",
    "                    print('+', end=' ') #positive reward terminal\n",
    "                else :\n",
    "                    print('-', end=' ') #-ve reward terminal\n",
    "                    \n",
    "            elif state not in self.policy :\n",
    "                print('#', end=' ') #walls\n",
    "            else :\n",
    "                print(print_chars[self.policy[state]], end=' ') #directions >, <, ^, v\n",
    "                \n",
    "            if (state[1]+1) % self.grid_world.columns == 0 :\n",
    "                print(\"\") #just a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(gamma=0.01, living_reward=-0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "* The value (utility) of a state s:\n",
    "V*(s) = expected utility starting in s and acting optimally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dict = {state: 0 for state in gw.all_states}\n",
    "def expected_value(gw, s, a, value_dict):\n",
    "    '''expected value in state N+1 knowing state N and intented move'''\n",
    "    exp_value = 0\n",
    "    states, actions, probs = gw.transitions(s, a)\n",
    "    for i, state in enumerate(states):\n",
    "        exp_value += probs[i] * (gw.reward(s, a) + value_dict[state])\n",
    "    return exp_value\n",
    "expected_value(gw, (0,3), Exit, value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gw):\n",
    "    '''returns the dictionary containing the values for each state after 100 iterations'''\n",
    "    value_dict = {state: 0 for state in gw.all_states}\n",
    "    N = 0\n",
    "    while N < 100:\n",
    "        for state in gw.all_states:\n",
    "            if state != (-1,-1):\n",
    "                list_values = []\n",
    "                for action in gw.actions(state):\n",
    "                    list_values.append(expected_value(gw, state, action, value_dict))\n",
    "                value_dict[state] = max(list_values)\n",
    "        N += 1\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = value_iteration(gw)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy extraction\n",
    "def policy_from_values(gw, value_dict, actions = Policy(gw)):\n",
    "    '''i.e. optimal action for each state that maximizes utility given a reward. \n",
    "    Replaces the optimal plan (sequence of actions) in deterministic search (= without probabilistic noise) '''\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            list_values = []\n",
    "            for action in gw.actions(state):\n",
    "                list_values.append((expected_value(gw, state, action, value_dict), action))\n",
    "            max_value, best_action = max(list_values) \n",
    "        actions[state] = best_action # np.argmax(list_values) if using numpy\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = policy_from_values(gw, V)\n",
    "start_state = (2, 0)\n",
    "rewards = 0\n",
    "end = False\n",
    "while end == 0:\n",
    "    next_state, reward, end = gw.move(start_state, a[start_state])\n",
    "    rewards += reward\n",
    "    start_state = next_state\n",
    "    print(next_state)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Value Iteration\n",
    "To avoid relooping through all the states, implement q-value iteration (q-value = exp_value(s, a, V(s')))<br>\n",
    "Very similar to value iteration<br>\n",
    "* The value (utility) of a state s:\n",
    "V*(s) = expected utility starting in s and acting optimally\n",
    "* The value (utility) of a q-state (s,a):\n",
    "Q*(s,a) = expected utility starting out having taken action a from state s and (thereafter) acting optimally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_dict = {(state, action): 0 for state in gw.all_states for action in gw.actions(state)}\n",
    "def expected_q_value(gw, s, q_value_dict):\n",
    "    '''returns the value for all the possible expected states given a current state '''\n",
    "    exp_q_value = {}\n",
    "    for a in gw.actions(s):\n",
    "        noise = gw.reward(s, a) + q_value_dict[(s, a)]\n",
    "        exp_q_value[a] = max([sublist * noise for sublist in gw.transitions(s,a)[2]])\n",
    "        q_value_dict[(s,a)] = exp_q_value[a]\n",
    "    return exp_q_value\n",
    "print(expected_q_value(gw, (2,0), q_value_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-value iteration\n",
    "def q_value_iteration(gw):\n",
    "    '''returns the dictionary containing the values for each combination of states and actions'''\n",
    "    value_dict = {state: 0 for state in gw.all_states}\n",
    "    N = 0\n",
    "    while N < 100:\n",
    "        for state in gw.all_states:\n",
    "            if state != (-1,-1):\n",
    "                list_values = expected_q_value(gw, state, q_value_dict)\n",
    "                for action in gw.actions(state):\n",
    "                    q_value_dict[(state, action)] += list_values[action]\n",
    "        N += 1\n",
    "    return q_value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy extraction\n",
    "# def policy_from_q_values(gw, q_value_dict, actions = Policy(gw)):\n",
    "#     for state in gw.all_states:\n",
    "#         if state != (-1,-1):\n",
    "#             list_values = []\n",
    "#             list_values.append(expected_q_value(gw, state, q_value_dict))\n",
    "#             max_value, best_action = max(list_values)\n",
    "#         actions[state] = best_action\n",
    "#     return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy iteration: more efficient since policy can be figured much before the values converge\n",
    "#1. Policy evaluation: calculate utilities for some fixed policy until convergence.\n",
    "#2. Policy improvement: update policy using one-step look-ahead with resulting converged (but not optimal) \n",
    "#utilities as future values\n",
    "#Repeat 1 and 2 until convergence\n",
    "#policy_value(s) = expected total discounted rewards starting in s and following that policy\n",
    "\n",
    "def values_from_policy(gw, actions, value_dict):\n",
    "    list_values = []\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            value_dict[state] = (expected_value(gw, state, actions[state], value_dict))\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = Policy(gw)\n",
    "for state in gw.all_states:\n",
    "    if state != (-1,-1):\n",
    "        actions[state] = gw.actions(state)[0]\n",
    "value_dict = {state: 0 for state in gw.all_states}\n",
    "values_from_policy(gw, actions, value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gw):\n",
    "    value_dict = {state: 0 for state in gw.all_states}\n",
    "    actions = Policy(gw)\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            actions[state] = gw.actions(state)[0]\n",
    "    N = 0\n",
    "    M = 0\n",
    "    while M < 100:\n",
    "        while N < 5:\n",
    "            values_check = value_dict.copy()\n",
    "            values = values_from_policy(gw, actions, value_dict)\n",
    "            N += 1\n",
    "        if values == values_check:\n",
    "            print(M, N)\n",
    "            break\n",
    "        actions = policy_from_values(gw, values)\n",
    "        M +=1\n",
    "        N = 0\n",
    "    return actions, values\n",
    "c, values = policy_iteration(gw)\n",
    "c.print()\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
