{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us have a gridworld\n",
    "#ref: Chapter 17, Artificial Intelligence a Modern Approach\n",
    "#ref: CS188 https://inst.eecs.berkeley.edu/~cs188/fa19/\n",
    "#ref: https://inst.eecs.berkeley.edu/~cs188/fa19/assets/slides/lec8.pdf\n",
    "#ref: https://courses.cs.washington.edu/courses/cse473/13au/slides/17-mdp-rl.pdf\n",
    "\n",
    "#This class will create a 2D grid of row x colums \n",
    "#Some of the cells can be disabled by putting it into walls\n",
    "#cells are addressed just like 2d arrays (r,c)\n",
    "#There are possibly many terminal states\n",
    "#terminal states have only one action available: Exit \n",
    "#Transistion is as per the book 80% action and 20%sideways ( a variable noise is used to control this distribution)\n",
    "#There is a special end state, (-1,-1), from which NO action is available. This state is used as a final state.\n",
    "\n",
    "#Actions #just some alias\n",
    "Up    = 0\n",
    "Down  = 1\n",
    "Right = 2\n",
    "Left  = 3\n",
    "Exit  = 4\n",
    "\n",
    "class GridWorld :\n",
    "    #Default is as given in the AIMA book\n",
    "    def __init__(self, \n",
    "                 rows    =3, \n",
    "                 columns =4, \n",
    "                 walls   =[(1,1)], terminals= {(0,3):+1.0, (1,3):-1.0}, \n",
    "                 gamma   =1.0, \n",
    "                 living_reward=0,\n",
    "                 noise   =0.2\n",
    "                ) :\n",
    "        \"\"\"We dont expect these parameters to change during the agent run\"\"\"\n",
    "        self.rows      = rows\n",
    "        self.columns   = columns\n",
    "        self.N         = rows * columns #total cells\n",
    "        self.walls     = walls\n",
    "        self.terminals = terminals #dictionary of terminal celss and their rewards.\n",
    "        self.gamma     = gamma\n",
    "        self.living_reward = living_reward\n",
    "        self.all_actions   = [ Up, Down, Right, Left, Exit ]\n",
    "        self.end_state     = (-1, -1) #a dummy state to reach after taking Exit\n",
    "        self.all_states    = [(r,c) for r in range(rows) for c in range(columns) if (r,c) not in walls ] + [self.end_state]\n",
    "        self.noise         = noise\n",
    "        \n",
    "        \n",
    "        #transitions from each state and the probabilities\n",
    "        self.noise                = noise\n",
    "        self.action_transitions   = { \n",
    "            Up:   ([Up,    Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Down: ([Down,  Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Left: ([Left,  Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Right:([Right, Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Exit :([Exit], [1.0])\n",
    "        }\n",
    "    \n",
    "    def actions(self, state) :\n",
    "        \"\"\"returns all valid actions from the current state\"\"\"\n",
    "        if state in self.terminals :\n",
    "            return [Exit]\n",
    "        if state == self.end_state :\n",
    "            return [] #No action available.\n",
    "        return [ Up, Down, Right, Left ]\n",
    "    \n",
    "    def reward(self, state, action, next_state=None) :\n",
    "        \"\"\"reward is the instantaneous reward. It is usually R(s,a,s')\"\"\"\n",
    "        #In grid world the reward depends only on state.\n",
    "        if state in self.terminals :\n",
    "            return self.terminals[state] #dict has the terminal values +1 or -1\n",
    "        if state == self.end_state :\n",
    "            return 0.0\n",
    "        return self.living_reward        #usually a small -ve value\n",
    "    \n",
    "    def transitions(self, state, action) :\n",
    "        \"\"\"return a list of tuple(nextstate, action, probability)\"\"\"\n",
    "        actual_actions, probs = self.action_transitions[action]\n",
    "        return [ self._next_cell(state, a) for a in actual_actions ], actual_actions, probs\n",
    "    \n",
    "    def move(self, state, action) :\n",
    "        \"\"\"Take the action and return the tuple(new_state, reward, is_terminal)\"\"\"                          \n",
    "        assert action in self.actions(state) #just a check if this is a valid action at this time or not\n",
    "        \n",
    "        cells, actions, p = self.transitions(state, action)\n",
    "        \n",
    "        #we choose one cell acccording to probabilities\n",
    "        new_state   = random.choices(cells, weights=p)[0] #only one; we take index 0                \n",
    "        reward      = self.reward(state, action) #\n",
    "        \n",
    "        is_terminal = False\n",
    "        if new_state == self.end_state :\n",
    "            is_terminal = True\n",
    "            \n",
    "        return new_state, reward, is_terminal #keep the same for mat as OpenAI gym.\n",
    "    \n",
    "    def _next_cell(self, state, action) : \n",
    "        \"\"\"Blindly takes the action without checking anything and returns the position\"\"\"\n",
    "        r,c = state #row & column\n",
    "        if action == Exit :\n",
    "            return self.end_state\n",
    "        if action == Up :\n",
    "            target = r-1, c  \n",
    "        if action == Down :\n",
    "            target = r+1, c\n",
    "        if action == Right :\n",
    "            target = r, c+1  \n",
    "        if action == Left :\n",
    "            target = r, c-1 \n",
    "        \n",
    "        if self._valid_cell(target) :\n",
    "            return target\n",
    "        return state #stay put the target is invalid.\n",
    "    \n",
    "    def _valid_cell(self, cell) :\n",
    "        \"\"\"Returns true if the cell is a valid cell\"\"\"\n",
    "        r, c = cell #this may be an illegal node; we need to check\n",
    "        \n",
    "        #is it any of the walls?\n",
    "        if (r,c) in self.walls :\n",
    "            return False\n",
    "        \n",
    "        #is it outside the grid?\n",
    "        if r < 0 or r >= self.rows or c < 0 or c >= self.columns :\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    #pretty print the grid and agent if given.\n",
    "    def print(self, agent_state=None) :\n",
    "        for r in range(self.rows) :\n",
    "            for c in range(self.columns) :\n",
    "                cell = (r,c)\n",
    "                if cell in self.walls :\n",
    "                    print('# ', end='')\n",
    "                elif cell in self.terminals :\n",
    "                    if self.terminals[cell] > 0 :\n",
    "                        print('+', end=' ')\n",
    "                    else :\n",
    "                        print('-', end=' ')\n",
    "                elif cell == agent_state :\n",
    "                    print('@ ', end='')\n",
    "                else :\n",
    "                    print('. ', end='')\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple class to hold the policy dictionary\n",
    "#useful for printing the policy and hiding some details.\n",
    "\n",
    "class Policy :\n",
    "    def __init__(self, grid_world=None) :\n",
    "        \"\"\"Holds one policy and returns actions according to it\"\"\"\n",
    "        self.grid_world = grid_world\n",
    "        self.policy     = { } #{ state: policy_action}\n",
    "        \n",
    "    def __getitem__(self, state) :\n",
    "        return self.policy[state]\n",
    "    \n",
    "    def __setitem__(self, state, action) :\n",
    "        self.policy[state] = action\n",
    "    \n",
    "    \n",
    "    #Just a pretty print function for easy debugging\n",
    "    def print(self) :\n",
    "        print_chars = {Up:'^', Down:'v', Right:'>', Left:'<', Exit:'+'}\n",
    "        for state in [(r,c) for r in range(self.grid_world.rows) for c in range(self.grid_world.columns)]:\n",
    "            \n",
    "            if state in self.grid_world.terminals :\n",
    "                if self.grid_world.terminals[state] >= 0 :\n",
    "                    print('+', end=' ') #positive reward terminal\n",
    "                else :\n",
    "                    print('-', end=' ') #-ve reward terminal\n",
    "                    \n",
    "            elif state not in self.policy :\n",
    "                print('#', end=' ') #walls\n",
    "            else :\n",
    "                print(print_chars[self.policy[state]], end=' ') #directions >, <, ^, v\n",
    "                \n",
    "            if (state[1]+1) % self.grid_world.columns == 0 :\n",
    "                print(\"\") #just a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(gamma=0.01, living_reward=-0.04)\n",
    "start = (2,0) #as in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_dict = {state: 0 for state in gw.all_states}\n",
    "def expected_value(gw, s, a, value_dict):\n",
    "    exp_value = 0\n",
    "    states, actions, probs = gw.transitions(s, a)\n",
    "    for i, state in enumerate(states):\n",
    "        exp_value += probs[i] * (gw.reward(s, a) + value_dict[state])\n",
    "    return exp_value\n",
    "expected_value(gw, (0,3), Exit, value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gw):\n",
    "    '''returns the dictionary containing the values for each state'''\n",
    "    value_dict = {state: 0 for state in gw.all_states}\n",
    "    N = 0\n",
    "    while N < 100:\n",
    "        for state in gw.all_states:\n",
    "            if state != (-1,-1):\n",
    "                list_values = []\n",
    "                for action in gw.actions(state):\n",
    "                    list_values.append(expected_value(gw, state, action, value_dict))\n",
    "                value_dict[state] = max(list_values)\n",
    "        N += 1\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.8115582191780822,\n",
       " (0, 1): 0.8678082191780823,\n",
       " (0, 2): 0.9178082191780822,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.7615582191780823,\n",
       " (1, 2): 0.6602739726027398,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.7053082191780823,\n",
       " (2, 1): 0.6553082191780822,\n",
       " (2, 2): 0.6114155251141553,\n",
       " (2, 3): 0.38792491121258255,\n",
       " (-1, -1): 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = value_iteration(gw)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy extraction\n",
    "def policy_from_values(gw, V):\n",
    "    actions = Policy(gw)\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            list_values = []\n",
    "            for action in gw.actions(state):\n",
    "                list_values.append((expected_value(gw, state, action, V), action))\n",
    "            max_value, best_action = max(list_values)\n",
    "        actions[state] = best_action\n",
    "    return actions\n",
    "a = policy_from_values(gw, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy extraction with numpy\n",
    "import numpy as np\n",
    "def policy_from_values(gw, V):\n",
    "    actions = Policy(gw)\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            list_values = []\n",
    "            for action in gw.actions(state):\n",
    "                list_values.append((expected_value(gw, state, action, V)))\n",
    "        actions[state] = np.argmax(list_values)\n",
    "    return actions\n",
    "a = policy_from_values(gw, V)\n",
    "#Note: to avoid relooping through all the states, implement q-value iteration (q-value = exp_value(s, a, V(s')))\n",
    "#Very similar to value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > + \n",
      "^ # > - \n",
      "> > > ^ \n"
     ]
    }
   ],
   "source": [
    "a.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0)\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(-1, -1)\n",
      "-9.0\n"
     ]
    }
   ],
   "source": [
    "start_state = (2, 0)\n",
    "rewards = 0\n",
    "end = False\n",
    "while end == 0:\n",
    "    next_state, reward, end = gw.move(start_state, a[start_state])\n",
    "    rewards += reward\n",
    "    start_state = next_state\n",
    "    print(next_state)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -0.04000000000000001,\n",
       " (0, 1): -0.044,\n",
       " (0, 2): -0.044399999999999995,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): -0.07200000000000002,\n",
       " (1, 2): -0.07552000000000002,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): -0.09760000000000002,\n",
       " (2, 1): -0.04976,\n",
       " (2, 2): -0.10539200000000001,\n",
       " (2, 3): -0.8505392,\n",
       " (-1, -1): 0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Policy iteration: more efficient since policy can be figured much before the values converge\n",
    "#1. Policy evaluation: calculate utilities for some fixed policy until convergence.\n",
    "#2. Policy improvement: update policy using one-step look-ahead with resulting converged (but not optimal) \n",
    "#utilities as future values\n",
    "#Repeat 1 and 2 until convergence\n",
    "#policy_value(s) = expected total discounted rewards starting in s and following that policy\n",
    "actions = Policy(gw)\n",
    "for state in gw.all_states:\n",
    "    if state != (-1,-1):\n",
    "        actions[state] = gw.actions(state)[0]\n",
    "value_dict = {state: 0 for state in gw.all_states}\n",
    "def values_from_policy(gw, actions, value_dict):\n",
    "    list_values = []\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            value_dict[state] = (expected_value(gw, state, actions[state], value_dict))\n",
    "    return value_dict\n",
    "values_from_policy(gw, actions, value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorld(gamma=0.01, living_reward=-2)\n",
    "start = (2,0) #as in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gw):\n",
    "    value_dict = {state: 0 for state in gw.all_states}\n",
    "    actions = Policy(gw)\n",
    "    for state in gw.all_states:\n",
    "        if state != (-1,-1):\n",
    "            actions[state] = gw.actions(state)[0]\n",
    "    N = 0\n",
    "    while N < 10:\n",
    "        values = values_from_policy(gw, actions, value_dict)\n",
    "        actions = policy_from_values(gw, values)\n",
    "        N += 1\n",
    "    return actions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > + \n",
      "^ # > - \n",
      "> > > ^ \n"
     ]
    }
   ],
   "source": [
    "c, values = policy_iteration(gw)\n",
    "c.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -7.042965366862879,\n",
       " (0, 1): -4.230087445001626,\n",
       " (0, 2): -1.7300525832514788,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): -9.543245731548135,\n",
       " (1, 2): -3.5704528159278244,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): -10.816037602817046,\n",
       " (2, 1): -8.474564126466234,\n",
       " (2, 2): -5.974447911839857,\n",
       " (2, 3): -3.7749391752645356,\n",
       " (-1, -1): 0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (games)",
   "language": "python",
   "name": "games"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
